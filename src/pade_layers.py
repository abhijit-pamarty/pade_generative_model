import torch
import torch.nn as nn
import torch.nn.functional as f
from config.load_configurations import load_exp_configs
import torch.optim as optim

#define the pade layer
class Pade_Layer_2D(nn.Module):
    
    def __init__(self, parameter_dim, num_X, num_Y):
        
        super(Pade_Layer_2D, self).__init__()
        
        self.exp_configs = load_exp_configs()

        #fc layers for pade approximant
        fc_1 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_1"]
        fc_2 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_2"]
        fc_3 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_3"]
        fc_4 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_4"]
        
        self.num_order = self.exp_configs["model"]["pade_layer"]["numerator_order"]                                #pade approximant numerator coefficients
        self.denom_order = self.exp_configs["model"]["pade_layer"]["denominator_order"]                            #pade approximant denominator coefficients
        
        
        #create the model
        self.fc1 = nn.Linear(in_features= parameter_dim, out_features= fc_1)
        self.fc2 = nn.Linear(in_features= fc_1, out_features= fc_2)
        self.fc3 = nn.Linear(in_features= fc_2, out_features= fc_3)
        self.fc4 = nn.Linear(in_features= fc_3, out_features= fc_4)
        
        self.M = num_X
        self.N = num_Y
        
        #construct the pade tensor in the numerator, before multiplying with the coefficients (creates a row vector)
        self.X_powers_num_L = nn.Linear(in_features= fc_4, out_features = self.num_order)
        self.Y_powers_num_L = nn.Linear(in_features= fc_4, out_features = self.num_order)
        
        
        #construct the pade tensor in the denominator, before multiplying with the coefficients (creates a row vector)
        self.X_powers_denom_L = nn.Linear(in_features= fc_4, out_features = self.denom_order)
        self.Y_powers_denom_L = nn.Linear(in_features= fc_4, out_features = self.denom_order)
        
        
        #generate the weights tensor for the dot product
        self.fc_num_weights = nn.Linear(in_features = fc_4, out_features = (self.num_order))
        self.fc_denom_weights = nn.Linear(in_features = fc_4, out_features = (self.denom_order))
        self.e = float(self.exp_configs["model"]["pade_layer"]["epsilon"])
        
        
        
        #fc layers for short connection
        fc_5 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_5"]
        fc_6 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_6"]

        #generate the short from input to output, to generate the average of the function
        self.fc6 = nn.Linear(in_features = parameter_dim, out_features = fc_5)
        self.fc7 = nn.Linear(in_features = fc_5, out_features = fc_6)
        self.fc8 = nn.Linear(in_features = fc_6, out_features = 1)
        
        #fc layers for scale
        fc_7 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_7"]
        fc_8 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_8"]
        
        
        #generate scale from input to output
        
        self.fc9 = nn.Linear(in_features = parameter_dim, out_features = fc_7)
        self.fc10 = nn.Linear(in_features = fc_7, out_features = fc_8)
        self.fc11 = nn.Linear(in_features = fc_8, out_features = 1)

    
        
    def forward(self, x, X, Y):
        
        ############## PADE APPROXIMANT ##############
        
        #X and Y must be matrices of shape [M, N] generated by meshgrid
        
        
        #FC layers
        x1 = f.leaky_relu(self.fc1(x))
        x2 = f.leaky_relu(self.fc2(x1))
        x3 = f.leaky_relu(self.fc3(x2))
        x4 = f.leaky_relu(self.fc4(x3))
        
        ### weights tensor ###
        
        num_coeffs = f.leaky_relu(self.fc_num_weights(x4))
        denom_coeffs = f.leaky_relu(self.fc_denom_weights(x4))
        
        ### power tensor ###
        
        ## numerator ##
        X_powers_num = f.sigmoid(self.X_powers_num_L(x4))*self.num_order
        Y_powers_num = f.sigmoid(self.Y_powers_num_L(x4))*self.num_order
        
        X_powers_num = X_powers_num.view(1, -1).unsqueeze(0)
        Y_powers_num = Y_powers_num.view(1, -1).unsqueeze(0)
        
        ## denominator ##
        X_powers_denom = f.sigmoid(self.X_powers_denom_L(x4))*self.denom_order
        Y_powers_denom = f.sigmoid(self.Y_powers_denom_L(x4))*self.denom_order
        
        X_powers_denom = X_powers_denom.view(1, -1).unsqueeze(0)
        Y_powers_denom = Y_powers_denom.view(1, -1).unsqueeze(0)
        
        
        
        #calculate the powers in one dimension
        #unsqueeze the array
        
        X = X.unsqueeze(-1)                                            #size = [M, N, 1]
        Y = Y.unsqueeze(-1)                                          #size = [M, N, 1]
        
        
        ##raise to the power 
        
        X_vals_num = X**X_powers_num                               #size = [M, N, num_order]
        Y_vals_num = Y**Y_powers_num                               #size = [M, N, num_order]
        X_vals_denom = X**X_powers_denom                           #size = [M, N, denom_order]
        Y_vals_denom = Y**Y_powers_denom                           #size = [M, N, denom_order]
        
        
        precoefficient_num = X_vals_num * Y_vals_num              #size = [M, N, num_order]
        precoefficient_denom = X_vals_denom * Y_vals_denom        #size = [M, N, denom_order]
        
        #unsqueeze the coefficient array
        num_coeffs = num_coeffs.unsqueeze(0)         #size = [1, 1, num_order]
        denom_coeffs = denom_coeffs.unsqueeze(0)     #size = [1, 1, denom_order]
        
        #calculate each term multiplied by the coefficient
        
        postcoefficient_num = num_coeffs * precoefficient_num           #size = [M, N, num_order]
        postcoefficient_denom = denom_coeffs * precoefficient_denom     #size = [M, N, denom_order]
        
        #sum along relevant dimensions
        sum_num = torch.sum(postcoefficient_num, (2))
        sum_denom = torch.sum(postcoefficient_denom, (2))
        
        #calculate the final pade approximant
        pade = torch.div(sum_num, sum_denom + self.e)    

        ############## SHORT CONNECTION ##############

        x5 = f.leaky_relu(self.fc6(x))
        x6 = f.leaky_relu(self.fc7(x5))
        short = f.leaky_relu(self.fc8(x6))
        
        ############## SCALE CONNECTION ###############
        
        x7 = f.leaky_relu(self.fc9(x))
        x8 = f.leaky_relu(self.fc10(x7))
        scale = torch.exp(self.fc11(x8))
        
        
        
        output = scale*pade + short
        
        return output
    
#define the pade layer
class Pade_Layer(nn.Module):
    
    def __init__(self, parameter_dim):
        
        super(Pade_Layer, self).__init__()
        
        self.exp_configs = load_exp_configs()

        #fc layers for pade approximant
        fc_1 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_1"]
        fc_2 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_2"]
        fc_3 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_3"]
        fc_4 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_4"]
        
        self.num_order = self.exp_configs["model"]["pade_layer"]["numerator_order"]                                #pade approximant numerator coefficients
        self.denom_order = self.exp_configs["model"]["pade_layer"]["denominator_order"]                            #pade approximant denominator coefficients
        self.batch_size = self.exp_configs["trainer"]["batch_size"]  
        
        #create the model
        self.fc1 = nn.Linear(in_features= parameter_dim, out_features= fc_1)
        self.fc2 = nn.Linear(in_features= fc_1, out_features= fc_2)
        self.fc3 = nn.Linear(in_features= fc_2, out_features= fc_3)
        self.fc4 = nn.Linear(in_features= fc_3, out_features= fc_4)
        
        #construct the pade tensor in the numerator, before multiplying with the coefficients (creates a row vector)

        self.dim = self.exp_configs["model"]["pade_layer"]["dimension"]
        self.integer_powers = self.exp_configs["model"]["pade_layer"]["integer_powers"]

        #create the numerator layers
        self.num_power_layers = nn.ModuleList(nn.Linear(in_features= fc_4, out_features = self.num_order) for layer_index in range(self.dim))
        
        #create the denominator layers
        self.denom_power_layers = nn.ModuleList(nn.Linear(in_features= fc_4, out_features = self.denom_order) for layer_index in range(self.dim))

        
        #generate the weights tensor for the dot product
        self.fc_num_weights = nn.Linear(in_features = fc_4, out_features = self.num_order)
        self.fc_denom_weights = nn.Linear(in_features = fc_4, out_features = self.denom_order)
        self.e = float(self.exp_configs["model"]["pade_layer"]["epsilon"])
        
        #fc layers for short connection
        fc_5 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_5"]
        fc_6 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_6"]

        #generate the short from input to output, to generate the average of the function
        self.fc6 = nn.Linear(in_features = parameter_dim, out_features = fc_5)
        self.fc7 = nn.Linear(in_features = fc_5, out_features = fc_6)
        self.fc8 = nn.Linear(in_features = fc_6, out_features = 1)
        
        #fc layers for scale
        fc_7 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_7"]
        fc_8 = self.exp_configs["model"]["pade_layer"]["num_layer_fc_8"]
        
        
        #generate scale from input to output
        
        self.fc9 = nn.Linear(in_features = parameter_dim, out_features = fc_7)
        self.fc10 = nn.Linear(in_features = fc_7, out_features = fc_8)
        self.fc11 = nn.Linear(in_features = fc_8, out_features = 1)

    def pade_approximant(self, X_vals_num, X_vals_denom, num_coeffs, denom_coeffs, num_powers, denom_powers):
        
        for dim_index in range(self.dim):

            #raise to the power
            X_vals_num[dim_index, :, :, :] = (X_vals_num[dim_index, :, :, :].clone()**num_powers[dim_index, :, :, :])
            X_vals_denom[dim_index, :, :, :] = (X_vals_denom[dim_index, :, :, :].clone()**denom_powers[dim_index, :, :, :])

        #calculate multiplication of values along all dimensions
        precoefficient_num = X_vals_num[0, :, :, :]
        precoefficient_denom = X_vals_denom[0, :, :, :]

        if self.dim > 1:
            for dim_index in range(self.dim - 1):

                precoefficient_num = precoefficient_num*X_vals_num[dim_index + 1, :, :, :].clone()
                precoefficient_denom = precoefficient_denom*X_vals_denom[dim_index + 1, :, :, :].clone()

        #unsqueeze the coefficient array
        num_coeffs = num_coeffs.unsqueeze(0).unsqueeze(2)        
        denom_coeffs = denom_coeffs.unsqueeze(0).unsqueeze(2)    

        #calculate each term multiplied by the coefficient
        postcoefficient_num = num_coeffs * precoefficient_num           #size = [M, N, num_order]
        postcoefficient_denom = denom_coeffs * precoefficient_denom     #size = [M, N, denom_order]

        #sum along relevant dimensions
        sum_num = torch.sum(postcoefficient_num, (3))
        sum_denom = torch.sum(postcoefficient_denom, (3))

        pade = torch.div(sum_num, sum_denom + self.e)

        return pade
        
    def forward(self, x, X, mode):
        
        #size of X: [dim, num_nodes, 1, 1] (here num_nodes is total number of nodes)

        if mode == "online":

            batch_size = 1
            
        else:

            batch_size = self.batch_size


        ############## PADE APPROXIMANT ##############
        
        #X and Y must be matrices of shape [M, N] generated by meshgrid
        
        
        #FC layers
        x1 = f.leaky_relu(self.fc1(x))
        x2 = f.leaky_relu(self.fc2(x1))
        x3 = f.leaky_relu(self.fc3(x2))
        x4 = f.leaky_relu(self.fc4(x3))
        
        #calculate the pade approximant
        ### weights tensor ###
        
        num_coeffs = f.leaky_relu(self.fc_num_weights(x4))
        denom_coeffs = f.leaky_relu(self.fc_denom_weights(x4))
        
        ### power tensor ###
        
        ## numerator ##
        num_powers = torch.zeros(num_coeffs.size()).unsqueeze(0).repeat(self.dim, 1, 1, 1).to(num_coeffs.device)
        denom_powers = torch.zeros(denom_coeffs.size()).unsqueeze(0).repeat(self.dim, 1, 1, 1).to(num_coeffs.device)

        X_vals_num = X.repeat(1, batch_size, 1, self.num_order)
        X_vals_denom = X.repeat(1, batch_size, 1, self.denom_order)

        for dim_index in range(self.dim):

            #get the powers
            num_powers[dim_index, :, :, :] = f.sigmoid(self.num_power_layers[dim_index](x4))*self.num_order
            denom_powers[dim_index, :, :, :] = f.sigmoid(self.denom_power_layers[dim_index](x4))*self.denom_order

        if mode == "train":
            num_powers = num_powers.squeeze().unsqueeze(2)
            denom_powers = denom_powers.squeeze().unsqueeze(2)

        if self.integer_powers:

            num_powers = torch.floor(num_powers)
            denom_powers = torch.floor(denom_powers)

        pade = self.pade_approximant(X_vals_num, X_vals_denom, num_coeffs, denom_coeffs, num_powers, denom_powers)

        ############## SHORT CONNECTION ##############

        x5 = f.leaky_relu(self.fc6(x))
        x6 = f.leaky_relu(self.fc7(x5))
        short = f.leaky_relu(self.fc8(x6))
        
        ############## SCALE CONNECTION ###############
        
        x7 = f.leaky_relu(self.fc9(x))
        x8 = f.leaky_relu(self.fc10(x7))
        scale = torch.exp(self.fc11(x8))
        
        
        output = scale*pade + short

        return output

def test():
    
    parameter_dim = 2
    model_test = Pade_Layer(parameter_dim)
    X = torch.zeros(2, 1, 10000, 1)
    x = torch.zeros(1, parameter_dim)
    model_test(x, X, "test")

# Example usage
if __name__ == "__main__":

    
    print("empty")